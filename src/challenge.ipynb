{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gcp import q1_bigquery\n",
    "from q1_time import q1_time, q1_time_pandas\n",
    "from q1_memory import q1_memory\n",
    "from q2_time import q2_time, q2_time_pandas\n",
    "from q2_memory import q2_memory\n",
    "from q3_time import q3_time, q3_time_pandas\n",
    "from q3_memory import q3_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enfoque general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grandes rasgos por cada pregunta se realiza lo siguente:\n",
    " - Optimización de tiempo de ejecución cargando los datos en un DataFrame\n",
    "     - Se usa la librería Pandas.\n",
    "     - Se usa librería Polars, dada su capacidad de procesar grandes volúmenes de datos de forma rápida y eficiente.\n",
    " - Optimización de uso de memoría leyendo el JSON línea a línea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La hipótesis es que para optimizar la velocidad resulta mejor cargar los datos a un Dataframe, ya sea de Pandas o Polars, debido a que están optimizados para trabajar de manera eficiente. De esta forma la manipulación de los datos y la aplicación de operaciones debería ser más rápida.\n",
    "\n",
    "Por otra parte, resulta evidente que para optimizar el uso de memoria cargar todos los datos a un Dataframe no es el camino correcto. Resulta mucho más conveniente leer cada tweet, iterando el JSON linea a línea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suposiciones\n",
    "- Se presupone que se tiene el archivo JSON en la misma carpeta que este notebook.\n",
    "- Se supone que el archivo ya está extraido del zip.\n",
    "- Para ejecutar la función basada en BigQuery se asume que ya se tiene un proyecto creado, con una cuenta de servicio con permisos de administrador de BigQuery y de objetos de storage (y disponemos de la key de la cuenta en formato JSON).\n",
    "- Se asume que se tiene un dataset en BigQuery con una tabla con los datos del JSON cargados.\n",
    "- Se asume que tenemos las variables de entorno *PROJECT_ID*, *KEYFILE_PATH*, *DATASET_ID* y *TABLE_NAME* en el entorno virtual para usar BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En el notebook **setup_gcp.ipynb** se explica más en detalle el preparamiento para usar BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener las top 10 fechas con más tweets y mencionar el usuario con más publicaciones en cada uno de esos días."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Optimización de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.0 Enfoque usando BigQuery de Google Cloud Platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea objeto Client de BigQuery a partir del *project_id* y el path de la *keyfile*.\n",
    "- Se arma la query SQL, reemplazando el *project_id*, *dataset_id* y *table_name* según los argumentos entregados a la función.\n",
    "- Se crea un BigQuery Job para ejecutar la query.\n",
    "- Se entrega resultado como lista de tuplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "keyfile_path = os.getenv(\"KEYFILE_PATH\")\n",
    "dataset_id = os.getenv(\"DATASET_ID\")\n",
    "table_name = os.getenv(\"TABLE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_bigquery(keyfile_path, project_id, dataset_id, table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 s ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_bigquery(keyfile_path, project_id, dataset_id, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 157.85 MiB, increment: 0.14 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_bigquery(keyfile_path, project_id, dataset_id, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Enfoque usando librería Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea LazyFrame para leer el JSON.\n",
    "- Se seleccionan de columnas a usar, tomando *date* y *username* (desde dentro de *user*).\n",
    "- Se agrega columna *date_count* con conteo de tweets por día (usando lógica *over* similar a funciones de venta en SQL).\n",
    "- Se agrega columna *user_count* con conteo tweets por combinacion de día y username (usando lógica *over*).\n",
    "- Se ordena según *user_count*, se agrupa según *date* y se extrae el primer username por cada partición, junto con *date_count* asociada.\n",
    "- Se ordena el resultado (una fila por día) por *date_count* descendiente y se extraen los primeros 10.\n",
    "- Se materializa el LazyFrame a DataFrame.\n",
    "- Se entrega resultado como lista de tuplas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.91 s ± 120 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1675.81 MiB, increment: 1524.47 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Enfoque usando librería Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se cargan de datos del JSON en un Dataframe.\n",
    "- Se seleccionan de columnas a usar, tomando date y username (desde dentro de user).\n",
    "- Se obtienen indices de los 10 días con más tweets y se filtran del dataframe.\n",
    "- Se agrega columna *count* con conteo tweets por combinacion de día y username (usando *group_by*).\n",
    "- Se obtienen los índices de usernames con más tweets por día haciendo *group_by* por día y extrayendo el índice de la fila con mayor *count* de cada partición.\n",
    "- Se extraen los usernames del Dataframe según índices encontrados, se elimina columna con conteo y se entrega salida como lista de tuplas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_time_pandas(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.65 s ± 445 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3247.39 MiB, increment: 2039.49 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimización de uso de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoque leyendo línea a línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se lee JSON entero línea por línea, creando diccionario con cada día distinto como *key* y el conteo de tweets del día como *value*.\n",
    "- Se ordena diccionario según conteo y se extraen los 10 días máximos.\n",
    "- Por cada uno de los 10 días máximos:\n",
    "    - Se lee JSON en línea por línea, creando diccionario con cada username como *key* y el conteo de tweets del user como *value*.\n",
    "    - Se ordena diccionario y se extrae el username con más publicaciones del día (se agrega como tupla a una lista de salida).\n",
    "- Se entrega lista resultante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cabe mencionar que se está leyendo el JSON línea por línea 11 veces, lo que obviamente es lento, pero se hace debido al enfoque en uso de memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La idea es evitar crear un diccionario con dias como keys y diccionario de usernames como values, ya que esta variable ocuparía gran cantidad de memoría.\n",
    "> Se prefiere iterar día a día extrayendo el username más mencionado, manteniendo en comparación una variable de menor tamaño que se sobreescribe en cada iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_memory(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 25s ± 818 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 148.39 MiB, increment: 1.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En cuanto al tiempo de ejecución se puede ver que usar Polars es un 1.7 veces más rápido que Pandas y 17 veces más rápido que el enfoque en optimización de memoria. Es 0.53 veces más lento que BigQuery.\n",
    "- En cuanto a uso de memoria, el enfoque leyendo línea a línea es 11 veces menor a usando Polars y 22 veces menor a usando Pandas. Ocupa aproximadamente la misma memoria que el enfoque usando BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los top 10 emojis más usados con su respectivo conteo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización de esta pregunta se utiliza la librería *emojis*.   \n",
    "Se consideró la posibilidad de encontrar los emojis usando expresiones regulares debido a su mayor velocidad, pero se termina usando librería emojis para tener mayor precisión en los resultados.\n",
    "\n",
    "> Se hace la suposición de que no se quieren agrupar emojis de la misma \"forma\" pero distinto color.\n",
    "> Por ejemplo, corazón rojo y verde o *praying hands* de distinto tono de piel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Optimización de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Enfoque usando librería Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea LazyFrame para leer el JSON.\n",
    "- Se seleccionan las columnas a usar, tomando *content* y aplicando una función para mapear *content* a una lista de emojis contenidos.\n",
    "- Se filtran tweets sin emojis.\n",
    "- Se materializa el LazyFrame a DataFrame.\n",
    "- Se \"abren\" las filas creando una fila nueva por cada emoji en la lista.\n",
    "- Se crea dataframe con conteo por emojis, se ordena de manera descendente y se extraen los 10 más usados.\n",
    "- Se entrega resultado como lista de tuplas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_time(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 s ± 181 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1034.31 MiB, increment: 386.59 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Enfoque usando librería Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se cargan de datos del JSON en un Dataframe.\n",
    "- Se seleccionan las columnas a usar, tomando *content* y aplicando una función para mapear *content* a una lista de emojis contenidos.\n",
    "- Se \"abren\" las filas creando una fila nueva por cada emoji en la lista.\n",
    "- Se crea dataframe con conteo por emojis, se ordena de manera descendente y se extraen los 10 más usados.\n",
    "- Se entrega resultado como lista de tuplas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5049, '🙏'), (3072, '😂'), (2972, '🚜'), (2182, '🌾'), (2086, '🇮🇳'), (1668, '🤣'), (1651, '✊'), (1382, '❤️'), (1317, '🙏🏻'), (1040, '💚')]\n"
     ]
    }
   ],
   "source": [
    "print(q2_time_pandas(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.7 s ± 532 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3620.69 MiB, increment: 2596.27 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_time_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Optimización de uso de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoque leyendo línea a línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea un contador *Counter()* para los emojis.\n",
    "- Se lee JSON entero línea por línea y por cada una:\n",
    "    - Se extrae campo *content* y se extrae lista de emojis con librería *emojis*.\n",
    "    - Por cada emoji en la lista se actualiza el contador de emojis.\n",
    "- Se encuentran los 10 emojis más usados con método *most_common(10)* y se entregan como salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_memory(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.2 s ± 670 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 149.76 MiB, increment: 1.38 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En cuanto al tiempo de ejecución se puede ver que usar Polars es un 1.23 veces más rápido que Pandas y 1.14 veces más rápido que el enfoque en optimización de memoria.\n",
    "- En cuanto a uso de memoria, el enfoque leyendo línea a línea es 6.9 veces menor a usando Polars y 24 veces menor a usando Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización de esta pregunta se utiliza el campo *mentionedUsers* de los tweets.   \n",
    "Se consideró la posibilidad de usar expresiones regulares haciendo uso del @, pero se prefirío usar el campo de usuarios mencionados, ya que probablemente entrega resultados más precisos (en teoría pueden haber @ no usados para referenciar a otros users, como en correos).\n",
    "\n",
    "> Se hace la suposición de que por cada tweet se menciona máximo una vez a cada *username* distinto (en teoría se puede mencionar más de una vez en un mismo tweet a alguien). Dado que es algo que no ocurre demasiado, probablemente no afecta los resultados finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Optimización de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Enfoque usando librería Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea LazyFrame para leer el JSON.\n",
    "- Se seleccionan las columnas a usar, tomando *mentionedUsers* y aplicando una función para mapear *mentionedUsers* a una lista de *usernames* contenidos.\n",
    "- Se filtran tweets sin menciones a otros usurios.\n",
    "- Se materializa el LazyFrame a DataFrame.\n",
    "- Se \"abren\" las filas creando una fila nueva por cada *username* en la lista.\n",
    "- Se crea dataframe con conteo por *usernames*, se ordena de manera descendente y se extraen los 10 más mencionados.\n",
    "- Se entrega resultado como lista de tuplas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "print(q3_time(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.91 s ± 320 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1398.32 MiB, increment: 461.33 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Enfoque usando librería Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se cargan de datos del JSON en un Dataframe.\n",
    "- Se seleccionan las columnas a usar, tomando *mentionedUsers* y aplicando una función para mapear *mentionedUsers* a una lista de *usernames* contenidos.\n",
    "- Se \"abren\" las filas creando una fila nueva por cada *username* en la lista.\n",
    "- Se crea dataframe con conteo por *usernames*, se ordena de manera descendente y se extraen los 10 más usados.\n",
    "- Se entrega resultado como lista de tuplas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2265, 'narendramodi'), (1840, 'Kisanektamorcha'), (1644, 'RakeshTikaitBKU'), (1427, 'PMOIndia'), (1146, 'RahulGandhi'), (1048, 'GretaThunberg'), (1019, 'RaviSinghKA'), (986, 'rihanna'), (962, 'UNHumanRights'), (926, 'meenaharris')]\n"
     ]
    }
   ],
   "source": [
    "print(q3_time_pandas(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.1 s ± 156 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3311.57 MiB, increment: 2658.90 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_time_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Optimización de uso de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoque leyendo línea a línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea un contador *Counter()* para los *usernames* mencionados.\n",
    "- Se lee JSON entero línea por línea y por cada una:\n",
    "    - Se extrae campo *mentionedUsers* y se extrae lista de *usernames*.\n",
    "    - Por cada *username* en la lista se actualiza el contador de *usernames*.\n",
    "- Se encuentran los 10 *usernames* más usados con método *most_common(10)* y se entregan como salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "print(q3_memory(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7 s ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de uso de memoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 151.32 MiB, increment: 1.56 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En cuanto al tiempo de ejecución se puede ver que el enfoque en optimización de memoria es 1.5 veces más rápido que Polars y 1.4 veces más rápido que Pandas.\n",
    "- En cuanto a uso de memoria, el enfoque leyendo línea a línea es 9.2 veces menor a usando Polars y 22 veces menor a usando Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se concluye que Polars es mucho más rápido para leer JSON separados por lineas gracias a su funcion scan_ndjson. El uso de LazyFrames realmente hace una diferencia comparado con la carga directa a un Dataframe de Pandas.\n",
    "\n",
    "- Se ve que el uso de memoría usando Polars es significativamente menor al de Pandas.\n",
    "\n",
    "- Se evidencia que cargar datos con estructura anidada y de un tamaño importante a un Dataframe no es una operación rápida.\n",
    "\n",
    "- Resulta interesante que el enfoque para optimizar el uso de memoria, leyendo línea por linea resulta más rápido en varios casos. Probablemente la razón es el tiempo de carga de los datos de Polars y sobre todo de Pandas.\n",
    "\n",
    "- El enfoque basado en BigQuery resulta mucho más rápido que el resto, pero la comparación es injusta, puesto que no se considera el tiempo de la carga de datos.\n",
    "\n",
    "- En cuanto a la optimización del uso de memoria se puede ver que leer línea por linea supera consistentemente a la carga de los datos en un dataset, como uno esperaría teóricamente.\n",
    "\n",
    "- Se puede ver que incluso la ejecución usando BigQuery (que debería ocupar muy poca memoria, ya que el trabajo lo hacen los servidores de Google) usa sobre 100MB, lo que indica que las funciones de optimización de memoría son muy eficientes, puesto que ocupan una memoría parecida.\n",
    "\n",
    "- Se puede ver que los distintos enfoques entregan los mismos resultados, lo que hace probable que esten correctos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Posibles mejoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una posible mejora es preprocesar los datos para cargarlos más rápidamente en los Dataframes de Pandas y Polars. Resulta interesante usar el formato parquet para guardar los Dataframes dada su eficiencia y velocidad. Puesto que la mayor parte del tiempo de ejecución usando Pandas se gasta en la carga, resulta interesante esta opción para comparar en iguales condiciones con Polars.\n",
    "\n",
    "- Una mejora sería usar librerías más completas (como cProfle) para evaluar el tiempo de ejecución, con el fin de descubrir posibles cuello de botella en las funciones. Sería una buena forma de confirmar la sospecha de los tiempo de carga en los Dataframe.\n",
    "\n",
    "- Otra posible mejora es agregar el tiempo de carga de los datos en el caso de BigQuery, ya que con Pandas y Polars es el factor que más afecta.\n",
    "\n",
    "- También sería bueno agregar mayor documentación sobre el setup de Google Cloud Platform, con cada paso explicado más detalladamente. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
